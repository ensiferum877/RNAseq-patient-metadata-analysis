{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29c922d8-27e3-4dfb-8c5f-f5b3ee505cde",
   "metadata": {},
   "source": [
    "# Generation of a centralized count data\n",
    "\n",
    "One of the initial challenges researchers face when analyzing RNA-seq data is **the consolidation of count data from multiple samples into a single comprehensive dataset**. This centralized dataset forms the foundation for downstream analyses, including the application of dimensionality reduction techniques, differential expression analysis, and pathway enrichment, which are critical for uncovering biological insights.\n",
    "\n",
    "However, the process of aggregating count data is fraught with potential pitfalls, such as ensuring the consistency of gene identifiers (Geneid) across all samples. Misalignment in gene order or identifiers can lead to erroneous results, making the initial data preparation phase both crucial and challenging.\n",
    "\n",
    "This notebook presents an approach to centralizing RNA-seq count data, developed to address these challenges efficiently. A pivotal step in our method is the meticulous verification of the **Geneid feature's consistency across all samples**. This crucial verification step allows us to employ a direct appending strategy on disk for integrating count data from multiple files. Unlike traditional data merging techniques, which often necessitate loading substantial datasets into memory—thereby significantly increasing memory usage—our approach ensures data is concatenated without extensive memory demands. By automating this process, we not only minimize manual intervention and the likelihood of errors but also establish a solid foundation for subsequent analyses with considerably lower memory consumption."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9355910f-1da6-42c2-88bf-ec5cb84f71b8",
   "metadata": {},
   "source": [
    "# 1 - Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0590249c-ec7e-4797-b746-7ecea8e9b842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a961880f-8b11-4cbb-9ba5-4f9e603be24a",
   "metadata": {},
   "source": [
    "# 2 - Setting Directories and Listing Files\n",
    "\n",
    "This section sets up the working environment by defining the directory containing the **RNA-seq count data files (working_dir)** and dynamically obtaining the current working directory (home_dir). It then lists all .txt files within the working_dir, ensuring that only relevant files are considered for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fce88d78-7041-4357-a38f-deb367c5f8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the working directory and home directory\n",
    "working_dir = 'PD_RNAseq_CountData'\n",
    "home_dir = os.getcwd()\n",
    "\n",
    "# List all .txt files in the specified directory\n",
    "filenames_lst = [file for file in os.listdir(os.path.join(home_dir, working_dir)) if file.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664009ab-4ca8-484f-b48b-c058f5fb4dc2",
   "metadata": {},
   "source": [
    "# 3 - Determining whether the order of Geneid is consistent across all the files\n",
    "\n",
    "This markdown cell introduces the first major analytical step: verifying the consistency of gene identifiers (Geneid) across all files. This is a crucial prerequisite for accurate data consolidation, as inconsistencies in gene order could lead to misaligned data and incorrect analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7511025b-ab8d-429e-a6d9-56ff56da4f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking Geneid order: 100%|██████████| 1049/1049 [21:37<00:00,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All .txt files have the same 'Geneid' order.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to read the 'Geneid' column from a file\n",
    "def read_geneid_column(file_path):\n",
    "    return pd.read_csv(file_path, delimiter='\\t', comment='#', usecols=['Geneid'])\n",
    "\n",
    "# Read the 'Geneid' column from the first file to establish a reference\n",
    "\n",
    "# Initialize variable to store the 'Geneid' order from the first file\n",
    "reference_geneid = None\n",
    "# Flag to indicate if all .txt files have the same 'Geneid' order\n",
    "same_order = True\n",
    "\n",
    "# Iterate over the filenames with a progress bar\n",
    "\n",
    "for file in tqdm(filenames_lst, desc='Checking Geneid order'):\n",
    "    if file.endswith('.txt'):  # Process only .txt files\n",
    "        file_path = os.path.join(home_dir, working_dir, file)\n",
    "        current_geneid = read_geneid_column(file_path)\n",
    "        \n",
    "        if reference_geneid is None:\n",
    "            reference_geneid = current_geneid  # Set the reference 'Geneid' from the first .txt file\n",
    "        else:\n",
    "            if not current_geneid.equals(reference_geneid):\n",
    "                same_order = False\n",
    "                break  # Exit the loop early if the order doesn't match\n",
    "\n",
    "# Print the result\n",
    "\n",
    "if same_order:\n",
    "    print(\"All .txt files have the same 'Geneid' order.\")\n",
    "else:\n",
    "    print(\"Not all .txt files have the same 'Geneid' order.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679beff2-5f37-45d6-b221-cda27d6f4a2c",
   "metadata": {},
   "source": [
    "# 4 - Defining functions \n",
    "\n",
    "To ensure the efficient handling and centralization of RNA-seq count data, this notebook employs two key functions: **read_and_process_file** and **concatenate_columns**. These functions are meticulously designed to address specific challenges in RNA-seq data analysis, emphasizing memory efficiency and data integrity.\n",
    "\n",
    "## read_and_process_file\n",
    "### Purpose and Design:\n",
    "This function is pivotal for preprocessing individual RNA-seq count files. Its primary role is to:\n",
    "\n",
    "Read each file: Leveraging pandas to handle tab-delimited count data, ensuring compatibility with common RNA-seq output formats.\n",
    "Pattern Matching: Utilize regular expressions to identify and rename the last column based on a specified pattern (e.g., matching specific sample identifiers). This is crucial for maintaining consistency and clarity in the dataset, especially when dealing with multiple samples or experimental conditions.\n",
    "Selective Column Inclusion: Conditionally include the Geneid column based on the function call. This flexibility is vital for the initial file processing (to retain Geneid) and subsequent files (to exclude Geneid and prevent duplication).\n",
    "Rationale:\n",
    "This function encapsulates the preprocessing logic, making the script adaptable to varying file formats and naming conventions. By abstracting this logic, we ensure that each file is processed consistently, laying a solid foundation for accurate data consolidation.\n",
    "\n",
    "## concatenate_columns\n",
    "\n",
    "### Purpose and Design:\n",
    "The concatenate_columns function orchestrates the centralization process by:\n",
    "\n",
    "Initiating with a Base File: It starts by processing the first file completely, including the Geneid column, to establish the baseline dataset.\n",
    "Progressive Concatenation: For each additional file, it processes and appends the data column-wise to the existing dataset. This step is crucial for building up the centralized table without resorting to memory-intensive merge operations.\n",
    "Efficient Disk Operations: By writing the processed data back to disk after each addition, the function avoids the significant memory overhead typically associated with loading and manipulating large datasets in memory.\n",
    "Rationale:\n",
    "The choice to append data directly on disk, guided by the initial verification of Geneid consistency, circumvents the need for memory-intensive data merging. This approach significantly reduces memory consumption, making the process more scalable and efficient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8fa0f9f8-29c1-496a-a1df-b00893a49b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming home_dir, working_dir, filenames_lst are defined\n",
    "def read_and_process_file(file_path, pattern, include_geneid=False):\n",
    "    # Your existing function for preprocessing\n",
    "    df = pd.read_csv(file_path, delimiter='\\t', comment='#')\n",
    "    last_col_name = df.columns[-1]\n",
    "    match = re.search(pattern, last_col_name)\n",
    "    new_col_name = match.group(0) if match else \"Counts\"\n",
    "    if include_geneid:\n",
    "        return df.loc[:, ['Geneid', last_col_name]].rename(columns={last_col_name: new_col_name})\n",
    "    else:\n",
    "        return df.loc[:, [last_col_name]].rename(columns={last_col_name: new_col_name})\n",
    "\n",
    "def concatenate_columns(base_file_path, additional_files_paths, output_file_path, pattern):\n",
    "    # Process the first file and write its content (including Geneid)\n",
    "    base_df = read_and_process_file(base_file_path, pattern, include_geneid=True)\n",
    "    base_df.to_csv(output_file_path, sep='\\t', index=False, mode='w')\n",
    "    \n",
    "    # Process each of the additional files with a progress bar\n",
    "    for file_path in tqdm(additional_files_paths, desc='Processing files'):\n",
    "        processed_df = read_and_process_file(file_path, pattern, include_geneid=False)\n",
    "        # Since we're only adding data columns, extract the column as a series to avoid alignment issues\n",
    "        data_series = processed_df.iloc[:, 0]\n",
    "        \n",
    "        # Read the current output and combine column-wise\n",
    "        current_df = pd.read_csv(output_file_path, sep='\\t')\n",
    "        current_df[data_series.name] = data_series.values\n",
    "        \n",
    "        # Write back to the disk\n",
    "        current_df.to_csv(output_file_path, sep='\\t', index=False, mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcfe1f5-b141-469a-8914-e11133f03ad8",
   "metadata": {},
   "source": [
    "# 5 - Writing data on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f42f102a-0010-49a4-aae8-f343a7333ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct definitions based on your setup\n",
    "pattern = r'5104-SL-\\d{4}'\n",
    "home_dir = os.getcwd()  # This should correctly point to /home/jovyan/work\n",
    "processed_dir = 'Processed data'  # Directory name for processed data\n",
    "working_dir = 'PD_RNAseq_CountData'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67766a42-520d-4172-a475-06d455bf3e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the output directory exists\n",
    "os.makedirs(os.path.join(home_dir, processed_dir), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3054a32b-efe4-4a81-9636-b0924a099616",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 1046/1046 [6:15:20<00:00, 21.53s/it] \n"
     ]
    }
   ],
   "source": [
    "# Define your filenames list and paths\n",
    "filenames_lst = sorted([file for file in os.listdir(os.path.join(home_dir, working_dir)) if file.endswith('.txt')])\n",
    "\n",
    "base_file_path = os.path.join(home_dir, working_dir, filenames_lst[0])\n",
    "additional_files_paths = [os.path.join(home_dir, working_dir, fname) for fname in filenames_lst[1:]]\n",
    "output_file_path = os.path.join(home_dir, processed_dir, 'final_output.tsv')\n",
    "\n",
    "# Concatenate columns with preprocessing\n",
    "concatenate_columns(base_file_path, additional_files_paths, output_file_path, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff66ff5-1f88-4598-b035-52a092df0185",
   "metadata": {},
   "source": [
    "# 6 - Minimal exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13452b03-563b-47c1-92a0-7a91988b6184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the file is 157.66 MB.\n"
     ]
    }
   ],
   "source": [
    "# Get the file size in bytes\n",
    "file_size_bytes = os.path.getsize(output_file_path)\n",
    "\n",
    "def convert_size(size_bytes):\n",
    "    for unit in ['bytes', 'KB', 'MB', 'GB', 'TB', 'PB', 'EB', 'ZB', 'YB']:\n",
    "        if size_bytes < 1024:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024\n",
    "    return f\"{size_bytes:.2f} YB\"\n",
    "\n",
    "# Use the convert_size function to print the file size in a more readable format\n",
    "print(f\"The size of the file is {convert_size(file_size_bytes)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa9c59f2-9733-43cf-84ef-bae8ef27fb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_df = pd.read_csv(output_file_path, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5327d3d2-bdec-4721-b98c-1872ee260278",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Geneid</th>\n",
       "      <th>5104-SL-3174</th>\n",
       "      <th>5104-SL-3180</th>\n",
       "      <th>5104-SL-3171</th>\n",
       "      <th>5104-SL-3175</th>\n",
       "      <th>5104-SL-4006</th>\n",
       "      <th>5104-SL-3990</th>\n",
       "      <th>5104-SL-3902</th>\n",
       "      <th>5104-SL-3885</th>\n",
       "      <th>5104-SL-3854</th>\n",
       "      <th>...</th>\n",
       "      <th>5104-SL-4679</th>\n",
       "      <th>5104-SL-3953</th>\n",
       "      <th>5104-SL-4761</th>\n",
       "      <th>5104-SL-4749</th>\n",
       "      <th>5104-SL-4580</th>\n",
       "      <th>5104-SL-4592</th>\n",
       "      <th>5104-SL-4611</th>\n",
       "      <th>5104-SL-2827</th>\n",
       "      <th>5104-SL-2875</th>\n",
       "      <th>5104-SL-2868</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ENSG00000223972.5</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENSG00000227232.5</td>\n",
       "      <td>55</td>\n",
       "      <td>50</td>\n",
       "      <td>80</td>\n",
       "      <td>66</td>\n",
       "      <td>198</td>\n",
       "      <td>154</td>\n",
       "      <td>84</td>\n",
       "      <td>48</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>63</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>85</td>\n",
       "      <td>147</td>\n",
       "      <td>176</td>\n",
       "      <td>163</td>\n",
       "      <td>130</td>\n",
       "      <td>75</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ENSG00000278267.1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>54</td>\n",
       "      <td>68</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ENSG00000243485.5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ENSG00000284332.1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1048 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Geneid  5104-SL-3174  5104-SL-3180  5104-SL-3171  5104-SL-3175  \\\n",
       "0  ENSG00000223972.5            24            15            15            20   \n",
       "1  ENSG00000227232.5            55            50            80            66   \n",
       "2  ENSG00000278267.1            10             3            13            16   \n",
       "3  ENSG00000243485.5             7             5             8             4   \n",
       "4  ENSG00000284332.1             0             0             0             0   \n",
       "\n",
       "   5104-SL-4006  5104-SL-3990  5104-SL-3902  5104-SL-3885  5104-SL-3854  ...  \\\n",
       "0            18            11             9             6             7  ...   \n",
       "1           198           154            84            48            39  ...   \n",
       "2            11             4            13             3             1  ...   \n",
       "3             0             0             0             0             0  ...   \n",
       "4             0             0             0             0             0  ...   \n",
       "\n",
       "   5104-SL-4679  5104-SL-3953  5104-SL-4761  5104-SL-4749  5104-SL-4580  \\\n",
       "0             6             8             4             7             1   \n",
       "1            63            63            48            85           147   \n",
       "2             7            16             8            11            16   \n",
       "3             0             0             0             0             0   \n",
       "4             0             0             0             0             0   \n",
       "\n",
       "   5104-SL-4592  5104-SL-4611  5104-SL-2827  5104-SL-2875  5104-SL-2868  \n",
       "0             3            20             6            17             4  \n",
       "1           176           163           130            75           101  \n",
       "2            54            68            17            29            20  \n",
       "3             0             1             0             0             0  \n",
       "4             0             0             0             0             0  \n",
       "\n",
       "[5 rows x 1048 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centralized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "23ca2b0f-dde2-4f5a-b9cf-49a5323f2a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 58780 genes and 1048 samples.\n"
     ]
    }
   ],
   "source": [
    "rows, columns = centralized_df.shape\n",
    "print(f\"The DataFrame has {rows} genes and {columns} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0049cbd4-c2a6-40ec-a53e-071689b7449b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geneid          0\n",
      "5104-SL-3174    0\n",
      "5104-SL-3180    0\n",
      "5104-SL-3171    0\n",
      "5104-SL-3175    0\n",
      "               ..\n",
      "5104-SL-4592    0\n",
      "5104-SL-4611    0\n",
      "5104-SL-2827    0\n",
      "5104-SL-2875    0\n",
      "5104-SL-2868    0\n",
      "Length: 1048, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Assess missing values in dataframe\n",
    "# Check for missing values in the entire DataFrame\n",
    "missing_values_per_column = centralized_df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values for each column\n",
    "print(missing_values_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f956dac8-4141-4a12-80e7-844ce3957351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of missing values in the centralized dataframe: 0\n"
     ]
    }
   ],
   "source": [
    "total_missing_values = missing_values_per_column.sum()\n",
    "print(f\"Total number of missing values in the centralized dataframe: {total_missing_values}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
